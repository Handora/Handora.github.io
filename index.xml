<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Never stop learning!</title>
    <link>http://handora.github.io/</link>
    <description>Recent content on Never stop learning!</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Handora</copyright>
    <lastBuildDate>Wed, 16 May 2018 15:35:50 +0800</lastBuildDate>
    
	<atom:link href="http://handora.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dynamo Paper Review</title>
      <link>http://handora.github.io/post/2018/05/2018-05-16-dynamo/</link>
      <pubDate>Wed, 16 May 2018 15:35:50 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/05/2018-05-16-dynamo/</guid>
      <description>Dynamo: Amazon&amp;rsquo;s Highly Available Key-value Store Buzzwords  Highly decentralized, loosely coupled, service-oriented architecture
 Consistent Hashing
 virtual nodes  Quorum-like technique
 R + W &amp;gt; N  Decentralized replica synchronization protocol
 anti-entropy (replica synchronization) protocol
a big word for synchronizing two replicas
 Merkle tree
  Object versioning
 syntactic reconciliation
new versions subsume the previous version(s), and the system itself can determine the authoritative version</description>
    </item>
    
    <item>
      <title>Chord: Scable P2P Lookup Service Paper Review</title>
      <link>http://handora.github.io/post/2018/05/2018-05-11-p2p-chord/</link>
      <pubDate>Fri, 11 May 2018 15:54:01 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/05/2018-05-11-p2p-chord/</guid>
      <description>Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications Buzzwords  P2P, DHT Consistent Hashing Finger Table Automatically Load Balance Stabilization  Summary The paper introduces the lookup service in the P2P applications named Chord. The Chord introduces the consistent hashing algorithm for each node only needs to remember O(log N) other nodes and resolves all lookup via O(log N) messages. What\&amp;rsquo;s more, it can also maintain the information as nodes join and leave the system in no more than n O(log^2^ N) messages even concurrently.</description>
    </item>
    
    <item>
      <title>Existential Consistency Paper Review</title>
      <link>http://handora.github.io/post/2018/05/2018-05-08-measuring-consistency/</link>
      <pubDate>Tue, 08 May 2018 20:10:11 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/05/2018-05-08-measuring-consistency/</guid>
      <description>Existential Consistency: Measuring and Understanding Consistency at Facebook Buzzwords  a trade-off between stronger forms of consistency and higher performance properties
 consistency monitoring system
 practical consistency monitoring system principle consistency monitoring system  φ-consistency
 Facebook&amp;rsquo;s Replicated Storage
 Database Two-Level Cache Graph data model  Local Consistency Models
 Linearizability Per-Object Sequential Consistency Read-After-Write Consistency   Summary The paper introduces the storage for giant read-heavy sites(such as Facebook&amp;rsquo;s TAO) and their technique to detect anomalies and monitor consistency.</description>
    </item>
    
    <item>
      <title>Query Processing with LLVM Paper Review</title>
      <link>http://handora.github.io/post/2018/04/2018-04-17-llvm/</link>
      <pubDate>Tue, 17 Apr 2018 16:55:54 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/04/2018-04-17-llvm/</guid>
      <description>Efficiently Compiling Efficient Query Plans for Modern Hardware Buzzwords  tuple at a time  Volcano iterator model MonetDB VectorWise, Peloton  operation at a time vector at a time Transpilation  Gcc compilation time may be long  JIT Compilatioon  push-based data-centric keep tuple in CPU register LLVM toolkit intermediate representation(IR)  HyPer\&amp;rsquo;s Adaptive Execution model  Summary For the in-memory DBMS, the CPU becomes the bottlenecks of CPU.</description>
    </item>
    
    <item>
      <title>PNUTS Paper Review</title>
      <link>http://handora.github.io/post/2018/04/2018-04-16-pnuts/</link>
      <pubDate>Mon, 16 Apr 2018 16:32:19 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/04/2018-04-16-pnuts/</guid>
      <description>PNUTS: Yahoo!&amp;rsquo;s Hosted Data Serving Platform Buzzwords  data storage  hashed table ordered table  geographically distributed service automated load-balancing and failover relaxed consistency  per-record timeline consistency  asynchronous notification  Pub-Sub Message System Yahoo! Message Broker(YMB)  hosting various consistency guarantee  read-any read-critical read-latest write test-and-set-write  scatter-gather engine  Summary The paper aims at Web Application which requires scalability, short response time even geographically distributed, high availability, fault tolerance, and relaxed consistency.</description>
    </item>
    
    <item>
      <title>In-memory OLTP and Future Improvements Paper review</title>
      <link>http://handora.github.io/post/2018/04/2018-04-13-future-oltp/</link>
      <pubDate>Fri, 13 Apr 2018 21:45:24 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/04/2018-04-13-future-oltp/</guid>
      <description>OLTP Through the Looking Glass, and What We Found There Buzzwords  main memory transaction processing performance measurement new architecture  logless single thread transaction less  TPC-C benchmark cluster computing trend  shared disk shared memory shared nothing   Summary Currently, many history of the DBMSs is dealing with the limitation of the hardware. The architecture of the many current DBMSs is also similar to the architecture in the 1970s.</description>
    </item>
    
    <item>
      <title>Bayou Paper Review</title>
      <link>http://handora.github.io/post/2018/04/2018-04-12-bayou/</link>
      <pubDate>Thu, 12 Apr 2018 17:21:14 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/04/2018-04-12-bayou/</guid>
      <description>Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System Buzzwords  weakly replicated storage system for bad network connectivity eventual consistency dependency checks conflict resolution rollback and redo in serialization order tentative and committed session guarantees anti-entropy, epidemic algorithms logical clocks for timestamp log instead of data  Summary The operations in the disconnected or weakly connected network are often valuable in the real world. Many real-world applications require the operations of immediate reads and writes of a local replica even disconnected just as Git or Dropbox.</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>http://handora.github.io/aboutme/</link>
      <pubDate>Fri, 06 Apr 2018 20:37:38 +0800</pubDate>
      
      <guid>http://handora.github.io/aboutme/</guid>
      <description>Who&amp;rsquo;s this guy? Hi, i&amp;rsquo;m QianChen.
I&amp;rsquo;m a weak system programmer enjoying in distributed storage system. For a quick overview, I&amp;rsquo;m currently:
 Learning distributed system which is much interesting and cool! Ricing Arch Linux+i3wm configuration. Enjoying to work on emacs. Playing with rasppberry pi to build lots of amazing things.  I also love tennis, animes and card games. I have played Yo-Gi-Oh! for more than seven years.</description>
    </item>
    
    <item>
      <title>Zookeeper Paper Summary</title>
      <link>http://handora.github.io/post/2018/04/2018-04-01-zookeeper/</link>
      <pubDate>Sun, 01 Apr 2018 18:00:25 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/04/2018-04-01-zookeeper/</guid>
      <description>ZooKeeper: Wait-free coordination for Internet-scale systems buzzword  wait-free property distributed lock services FIFO client ordering asynchronous linearizability linearizable write atomic broadcast protocol  Zab dominated by reads read locally  Znode  Regular Ephemeral sequential flag  use zookeeper to implement more powerful primitives Herd effect Fetching service based on zookeeper linearizability vs. serializability  Summary This paper describes the ZooKeeper as a coordination kernel which provides high-performance service can be used to implement more powerful primitives.</description>
    </item>
    
    <item>
      <title>Microprocess Evolution Paper Summary</title>
      <link>http://handora.github.io/post/2018/03/2018-03-31-microprocess-evolution/</link>
      <pubDate>Sat, 31 Mar 2018 23:42:12 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/03/2018-03-31-microprocess-evolution/</guid>
      <description>Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution buzzword  evolution  new requirements bottlenecks good fortune  a science of tradeoffs the tradeoff between high performance and low costs the tradeoff between high performance and power awareness pipelines in-chip cache branch prediction Specialized Functional Units out of order processing clusters chip multiprocessors(CMP) simultaneous multithreading(SMT) asynchronous and synchronous units coexisting  summary This paper introduces the evolution of the microprocessor from 1971 to 2001 and give the opinion about the result of evolution.</description>
    </item>
    
    <item>
      <title>Kvscan学习体验</title>
      <link>http://handora.github.io/post/2018/03/2018-03-30-kvscan/</link>
      <pubDate>Fri, 30 Mar 2018 23:21:38 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/03/2018-03-30-kvscan/</guid>
      <description> KVscan 实现经历 库的选择 这次实现中完全都在学新的东西,一下介绍一下这次学习到的东西以及使用到的库
 Gtest
Google的单元测试库十分好用以及简单,唯一麻烦的地方是和cmake的配合,通 过网上看别人写的脚本大致学习了一下, 可以说单元测试帮助我将自己的项目 细分方便调试和梳理.
 cmake
cmake正好是最近在学的东西,可以兼容emacs的irony-mode插件方便自己编写 project,但是很是复杂,这话了我很长时间来学习,但不得不说cmake的项目构 建很帮助我认清这个项目的细节.
 rpclib
rpclib是一个简单而高效的rpc组件,我将它组合在了我的cmake,它提供了异步 和同步两种API也方便我提供两种API接口,我有一部分未完成的细节就是rpc超 时的判断,通过boost::asio的timer可以实现,因为时间问题,作为一个TODO.
  优化实现 我一共想到了三个可以优化的地方,实现了其中两个,以及有一些细节优化未处理,在 这篇中阐述一下
 服务器端内存优化
由于服务器端的数据量很大,需要考虑到内存管理,这里我联想到了DBMS的 BufferPoolManager,因为作为服务器应该比OS更加清楚自己的内存管理.
所以我选择了使用页的方式来管理,页选择了512B来简化实现,可以很好地进行 batch操作,提高IO.缺点在于插入和删除,现在没有很好的解决方案.我想到的 最好方法是用B+树来代替BST.也就是数据库里index的实现方式.
在evict算法方面我选择了lru,首先因为可以实现所有操作的O(1),再者以前写 过类似的代码可以复用.但是,我也考虑过这个算法的劣势,没有很好的适应总 是顺序调用这个条件,应该由更好的算法来适应这个算法.
我想到了一系列的小优化,比如说总是把前N个页放在BufferPool里面不 evict.这一部分没有实现,但其实也不难,给page增加一个参数即可,但在插入 时需要动态增改,比较麻烦.
还有一个优化是meta tree,因为每次我要找页时需要fetchPage,对于BST来说 一次NextPage可能要调用多次,所以我选择只把需要的meta data比如说父亲, 左右子树指针存放在内存里,这对于内存读取是很大的优化.
 网络传输优化
传输只传一个K/V是很浪费贷款的,所以我选择了使用页的方式来传输.首先可 以复用页的代码.第二可以进行batch传输,增大IO.当然512B可能有点过小,我 们可以增加页大小来进行适应,这在编程改进方面其实非常简单.
 客户端缓存优化
客户端缓存优化其实是一个非常大的优化方式,比前两者可能有更大的性能优 化,但是由于时间原因没有实现,这里讲述一下实现流程.我们完全可以将 BufferPoolManager那段代码加以复用,加上以前前N不让移出BufferPool的算 法,可以很大提升速度,编写方式也很是简单.
  项目中学习到的东西  我在项目学习到了很多知识,比如说代码复用,单元测试来减少总体测试的困难,模 块化,文档编写,缓存相对于超大型数据的优势以及快速学习及应用.这些也是 我在实现我的SimpleDb以及QCTP时遇到的问题.  </description>
    </item>
    
    <item>
      <title>Munin Summary</title>
      <link>http://handora.github.io/post/2018/03/2018-03-27-munin/</link>
      <pubDate>Mon, 26 Mar 2018 22:18:15 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/03/2018-03-27-munin/</guid>
      <description>Implementation and Performance of Munin buzzword  multiple consistency protocols release consistency shared program variables  requires memory to be consistent only at the specific synchronization points  delayed update queue  buffers and merges pending outgoing writes  prototype  preprocessor modified linker library routines os  Protocol Parameters &amp;amp; Sharing Annotations distributed queue-based synchronization protocol Data Object Directory Delayed Update Queue  Brief Summary Instead of one big, expensive shared-memory multiprocessor, data center with many inexpensive shared-memory multiprocessors is now an difficult while hot approach.</description>
    </item>
    
    <item>
      <title>Gfs Paper Notes</title>
      <link>http://handora.github.io/post/2018/03/2018-03-26-gfs/</link>
      <pubDate>Sun, 25 Mar 2018 01:21:26 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/03/2018-03-26-gfs/</guid>
      <description>The Google File System Different point against traditional distributed file system  component failures is normal files are huge most files are mutated by appending new data rather than overwriting flexibale API  Assumptions for design overview  often fail large files workloads  large streaming reads small random reads large, sequential writes modify is rare  semantics for multiple clients concurrently append High bandwith than low latency  API  usual operation snapshot  COW  record append  Architecture  single master, multiple chunkservers accessed by multiple clients files are divided into fixed-size chunks, identified by an unique chunk handle, also replicated  Chunk size: 64MB  master maintaains all file system metadata  namespace access control information mapping from files to chunks current location of chunks chunk lease management gc chunk migration Heart beats with chunkservers  Not POSIX API master just transmit metadata, all data-bearing communication goes to chunkservers No cache for file data  Metadata in master  Three major type  namespace(persisted, replicated) mapping from files to chunks(persisted, replicated) location of each chun\&amp;rsquo;s replicas(ask for information)  All in memory chunk location  poll at start   Operation log Consistency Model  Weak Consistency Strong Consistency  implicaiton for application level  atomically rename checkpoint checksum and unqiue identifier for padding and rare depulication  Leases  primary, one of the replications  Atomic Record Appends  successful record append is defined while intervening regions are undefined at-least-once, with predictable magic number and unique IDs  Master Operation  Namespace and locking if it involves d1/d2&amp;hellip;/dn/leaf, it will acquire read-locks on the  directory names /d1, /d1/d2, &amp;hellip;, d1/d2&amp;hellip;/dn, and either a read lockor a write lockon the full pathname d1/d2&amp;hellip;/dn/leaf</description>
    </item>
    
    <item>
      <title>Spark Paper Notes</title>
      <link>http://handora.github.io/post/2018/03/2018-03-25-spark/</link>
      <pubDate>Sat, 24 Mar 2018 21:31:09 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/03/2018-03-25-spark/</guid>
      <description>Resilient Distributed Datasets Note Question of MapReduce  lack abstraction for leveraging distributed memory  iterative algorithm interactive data mining   Resilient distributed datasets  readonly, partition collection of record coarse-grained transformations lineage api  RDD vs. DSM  recovery is more light weight backup tasks like Mapreduce for stragglers data locality  Not suitable for RDD  fine-grained updates application  Spark programming interface  driver with cluster of =workers  Example  Logistic Regression  // parse text into point, then persist it to RAM val points = spark.</description>
    </item>
    
  </channel>
</rss>