<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper Review on Never stop learning!</title>
    <link>http://handora.github.io/tags/paper-review/</link>
    <description>Recent content in Paper Review on Never stop learning!</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Handora</copyright>
    <lastBuildDate>Tue, 08 May 2018 20:10:11 +0800</lastBuildDate>
    
	<atom:link href="http://handora.github.io/tags/paper-review/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Existential Consistency Paper Review</title>
      <link>http://handora.github.io/post/2018/05/2018-05-08-measuring-consistency/</link>
      <pubDate>Tue, 08 May 2018 20:10:11 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/05/2018-05-08-measuring-consistency/</guid>
      <description>Existential Consistency: Measuring and Understanding Consistency at Facebook Buzzwords  a trade-off between stronger forms of consistency and higher performance properties
 consistency monitoring system
 practical consistency monitoring system principle consistency monitoring system  Ï†-consistency
 Facebook&amp;rsquo;s Replicated Storage
 Database Two-Level Cache Graph data model  Local Consistency Models
 Linearizability Per-Object Sequential Consistency Read-After-Write Consistency   Summary The paper introduces the storage for giant read-heavy sites(such as Facebook&amp;rsquo;s TAO) and their technique to detect anomalies and monitor consistency.</description>
    </item>
    
    <item>
      <title>Query Processing with LLVM Paper Review</title>
      <link>http://handora.github.io/post/2018/04/2018-04-17-llvm/</link>
      <pubDate>Tue, 17 Apr 2018 16:55:54 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/04/2018-04-17-llvm/</guid>
      <description>Efficiently Compiling Efficient Query Plans for Modern Hardware Buzzwords  tuple at a time  Volcano iterator model MonetDB VectorWise, Peloton  operation at a time vector at a time Transpilation  Gcc compilation time may be long  JIT Compilatioon  push-based data-centric keep tuple in CPU register LLVM toolkit intermediate representation(IR)  HyPer\&amp;rsquo;s Adaptive Execution model  Summary For the in-memory DBMS, the CPU becomes the bottlenecks of CPU.</description>
    </item>
    
    <item>
      <title>PNUTS Paper Review</title>
      <link>http://handora.github.io/post/2018/04/2018-04-16-pnuts/</link>
      <pubDate>Mon, 16 Apr 2018 16:32:19 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/04/2018-04-16-pnuts/</guid>
      <description>PNUTS: Yahoo!&amp;rsquo;s Hosted Data Serving Platform Buzzwords  data storage  hashed table ordered table  geographically distributed service automated load-balancing and failover relaxed consistency  per-record timeline consistency  asynchronous notification  Pub-Sub Message System Yahoo! Message Broker(YMB)  hosting various consistency guarantee  read-any read-critical read-latest write test-and-set-write  scatter-gather engine  Summary The paper aims at Web Application which requires scalability, short response time even geographically distributed, high availability, fault tolerance, and relaxed consistency.</description>
    </item>
    
    <item>
      <title>In-memory OLTP and Future Improvements Paper review</title>
      <link>http://handora.github.io/post/2018/04/2018-04-13-future-oltp/</link>
      <pubDate>Fri, 13 Apr 2018 21:45:24 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/04/2018-04-13-future-oltp/</guid>
      <description>OLTP Through the Looking Glass, and What We Found There Buzzwords  main memory transaction processing performance measurement new architecture  logless single thread transaction less  TPC-C benchmark cluster computing trend  shared disk shared memory shared nothing   Summary Currently, many history of the DBMSs is dealing with the limitation of the hardware. The architecture of the many current DBMSs is also similar to the architecture in the 1970s.</description>
    </item>
    
    <item>
      <title>Bayou Paper Review</title>
      <link>http://handora.github.io/post/2018/04/2018-04-12-bayou/</link>
      <pubDate>Thu, 12 Apr 2018 17:21:14 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/04/2018-04-12-bayou/</guid>
      <description>Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System Buzzwords  weakly replicated storage system for bad network connectivity eventual consistency dependency checks conflict resolution rollback and redo in serialization order tentative and committed session guarantees anti-entropy, epidemic algorithms logical clocks for timestamp log instead of data  Summary The operations in the disconnected or weakly connected network are often valuable in the real world. Many real-world applications require the operations of immediate reads and writes of a local replica even disconnected just as Git or Dropbox.</description>
    </item>
    
    <item>
      <title>Zookeeper Paper Summary</title>
      <link>http://handora.github.io/post/2018/04/2018-04-01-zookeeper/</link>
      <pubDate>Sun, 01 Apr 2018 18:00:25 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/04/2018-04-01-zookeeper/</guid>
      <description>ZooKeeper: Wait-free coordination for Internet-scale systems buzzword  wait-free property distributed lock services FIFO client ordering asynchronous linearizability linearizable write atomic broadcast protocol  Zab dominated by reads read locally  Znode  Regular Ephemeral sequential flag  use zookeeper to implement more powerful primitives Herd effect Fetching service based on zookeeper linearizability vs. serializability  Summary This paper describes the ZooKeeper as a coordination kernel which provides high-performance service can be used to implement more powerful primitives.</description>
    </item>
    
    <item>
      <title>Microprocess Evolution Paper Summary</title>
      <link>http://handora.github.io/post/2018/03/2018-03-31-microprocess-evolution/</link>
      <pubDate>Sat, 31 Mar 2018 23:42:12 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/03/2018-03-31-microprocess-evolution/</guid>
      <description>Requirements, Bottlenecks, and Good Fortune: Agents for Microprocessor Evolution buzzword  evolution  new requirements bottlenecks good fortune  a science of tradeoffs the tradeoff between high performance and low costs the tradeoff between high performance and power awareness pipelines in-chip cache branch prediction Specialized Functional Units out of order processing clusters chip multiprocessors(CMP) simultaneous multithreading(SMT) asynchronous and synchronous units coexisting  summary This paper introduces the evolution of the microprocessor from 1971 to 2001 and give the opinion about the result of evolution.</description>
    </item>
    
    <item>
      <title>Munin Summary</title>
      <link>http://handora.github.io/post/2018/03/2018-03-27-munin/</link>
      <pubDate>Mon, 26 Mar 2018 22:18:15 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/03/2018-03-27-munin/</guid>
      <description>Implementation and Performance of Munin buzzword  multiple consistency protocols release consistency shared program variables  requires memory to be consistent only at the specific synchronization points  delayed update queue  buffers and merges pending outgoing writes  prototype  preprocessor modified linker library routines os  Protocol Parameters &amp;amp; Sharing Annotations distributed queue-based synchronization protocol Data Object Directory Delayed Update Queue  Brief Summary Instead of one big, expensive shared-memory multiprocessor, data center with many inexpensive shared-memory multiprocessors is now an difficult while hot approach.</description>
    </item>
    
    <item>
      <title>Gfs Paper Notes</title>
      <link>http://handora.github.io/post/2018/03/2018-03-26-gfs/</link>
      <pubDate>Sun, 25 Mar 2018 01:21:26 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/03/2018-03-26-gfs/</guid>
      <description>The Google File System Different point against traditional distributed file system  component failures is normal files are huge most files are mutated by appending new data rather than overwriting flexibale API  Assumptions for design overview  often fail large files workloads  large streaming reads small random reads large, sequential writes modify is rare  semantics for multiple clients concurrently append High bandwith than low latency  API  usual operation snapshot  COW  record append  Architecture  single master, multiple chunkservers accessed by multiple clients files are divided into fixed-size chunks, identified by an unique chunk handle, also replicated  Chunk size: 64MB  master maintaains all file system metadata  namespace access control information mapping from files to chunks current location of chunks chunk lease management gc chunk migration Heart beats with chunkservers  Not POSIX API master just transmit metadata, all data-bearing communication goes to chunkservers No cache for file data  Metadata in master  Three major type  namespace(persisted, replicated) mapping from files to chunks(persisted, replicated) location of each chun\&amp;rsquo;s replicas(ask for information)  All in memory chunk location  poll at start   Operation log Consistency Model  Weak Consistency Strong Consistency  implicaiton for application level  atomically rename checkpoint checksum and unqiue identifier for padding and rare depulication  Leases  primary, one of the replications  Atomic Record Appends  successful record append is defined while intervening regions are undefined at-least-once, with predictable magic number and unique IDs  Master Operation  Namespace and locking if it involves d1/d2&amp;hellip;/dn/leaf, it will acquire read-locks on the  directory names /d1, /d1/d2, &amp;hellip;, d1/d2&amp;hellip;/dn, and either a read lockor a write lockon the full pathname d1/d2&amp;hellip;/dn/leaf</description>
    </item>
    
    <item>
      <title>Spark Paper Notes</title>
      <link>http://handora.github.io/post/2018/03/2018-03-25-spark/</link>
      <pubDate>Sat, 24 Mar 2018 21:31:09 +0800</pubDate>
      
      <guid>http://handora.github.io/post/2018/03/2018-03-25-spark/</guid>
      <description>Resilient Distributed Datasets Note Question of MapReduce  lack abstraction for leveraging distributed memory  iterative algorithm interactive data mining   Resilient distributed datasets  readonly, partition collection of record coarse-grained transformations lineage api  RDD vs. DSM  recovery is more light weight backup tasks like Mapreduce for stragglers data locality  Not suitable for RDD  fine-grained updates application  Spark programming interface  driver with cluster of =workers  Example  Logistic Regression  // parse text into point, then persist it to RAM val points = spark.</description>
    </item>
    
  </channel>
</rss>